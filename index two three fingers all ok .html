<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <title>HF Minimal Granulator</title>
  <link rel="manifest" href="manifest.json">
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      background-color: #000; color: #fff; height: 100vh; width: 100vw; overflow: hidden;
      position: relative; touch-action: none;
    }
    #app { height: 100%; width: 100%; position: relative; }
    #waveform { width: 100%; height: 100%; position: absolute; top: 0; left: 0; }
    #canvas { width: 100%; height: 100%; position: absolute; top: 0; left: 0; z-index: 1; }
    #spectrogram { width: 100%; height: 100%; position: absolute; top: 0; left: 0; display: none; z-index: 2; }
    .record-button {
      position: fixed; bottom: 20px; left: 50%; transform: translateX(-50%);
      width: 60px; height: 60px; border-radius: 50%; background-color: #f44336;
      border: none; cursor: pointer; z-index: 10; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);
    }
    .record-button.recording { animation: pulse 1.5s infinite; }
    @keyframes pulse {
      0% { transform: translateX(-50%) scale(1); }
      50% { transform: translateX(-50%) scale(1.1); }
      100% { transform: translateX(-50%) scale(1); }
    }
    .status {
      position: fixed; bottom: 10px; left: 10px; padding: 5px 10px; background: rgba(0, 0, 0, 0.5);
      border-radius: 4px; z-index: 10;
    }
    .mode-indicator, .noise-indicator {
      position: fixed; top: 10px; padding: 5px 10px; color: #fff;
      background: none; border: none; cursor: pointer; z-index: 10;
      text-decoration: none; user-select: none; -webkit-user-select: none; touch-action: manipulation;
    }
    .mode-indicator { right: 40px; }
    .noise-indicator { right: 10px; }
    .noise-indicator.calibrating { color: #f44336; }
    .region { position: absolute; height: 100%; background: rgba(255, 255, 255, 0.3); border: 1px solid rgba(255, 255, 255, 0.7); pointer-events: none; z-index: 3; transition: opacity 0.4s ease; }
    .handle { position: absolute; width: 1px; height: 100%; top: 0; cursor: ew-resize; pointer-events: all; z-index: 4; transition: opacity 0.4s ease; }
    .handle-start { left: 0; background: rgba(0, 255, 0, 0.7); }
    .handle-end { right: 0; background: rgba(255, 0, 0, 0.7); }
    body.fullscreen #canvas, body.fullscreen #spectrogram { width: 100vw; height: 100vh; }
    body.fullscreen .record-button { bottom: 30px; width: 80px; height: 80px; }
    @media (max-width: 768px) {
      .record-button { width: 80px; height: 80px; bottom: 30px; }
      .status, .mode-indicator, .noise-indicator { font-size: 14px; }
    }
  </style>
</head>
<body>
  <div id="app">
    <div id="waveform"></div>
    <canvas id="canvas"></canvas>
    <canvas id="spectrogram"></canvas>
    <button id="record-button" class="record-button"></button>
    <div id="status" class="status">Ready</div>
    <div id="mode-indicator" class="mode-indicator">W</div>
    <div id="noise-indicator" class="noise-indicator">N</div>
  </div>

  <script>
    let audioContext = null, audioBuffer = null, mediaRecorder = null, audioChunks = [], mediaStream = null;
    let isRecording = false, isPlaying = false, selectedRegion = { start: 0, end: 0 }, sourceNode = null;
    let isFirstClick = true, firstClickTime = null, isWaveformMode = true, analyser = null;
    const toFadeOut = 400;
    const kalmanFactor = 0; // Max noise reduction (0 = full removal, 1 = none)
    let gainNode = null;
    let isSilenced = false;
    let noiseProfile = null, isCalibrating = false;
    let twoFingerDistance = null;

    const recordButton = document.getElementById('record-button');
    const statusEl = document.getElementById('status');
    const canvas = document.getElementById('canvas'), ctx = canvas.getContext('2d');
    const waveformEl = document.getElementById('waveform');
    const spectrogram = document.getElementById('spectrogram'), spectrogramCtx = spectrogram.getContext('2d');
    const modeIndicator = document.getElementById('mode-indicator');
    const noiseIndicator = document.getElementById('noise-indicator');

    document.addEventListener('DOMContentLoaded', init);

    async function init() {
      resizeCanvas(); window.addEventListener('resize', resizeCanvas);
      try {
        const AudioContext = window.AudioContext || window.webkitAudioContext;
        audioContext = new AudioContext();
        gainNode = audioContext.createGain();
        analyser = audioContext.createAnalyser(); analyser.fftSize = 2048; analyser.smoothingTimeConstant = 0;
        if (/iPad|iPhone|iPod/.test(navigator.userAgent)) document.addEventListener('touchstart', resumeAudioContext, { once: true });
        updateStatus('Ready');
      } catch (err) { updateStatus('Audio not supported'); console.error('Audio init error:', err); }

      recordButton.addEventListener('click', handleRecordButtonClick);
      recordButton.addEventListener('touchend', (e) => { e.preventDefault(); handleRecordButtonClick(); });

      modeIndicator.addEventListener('click', toggleVisualizationMode);
      modeIndicator.addEventListener('touchend', (e) => { e.preventDefault(); toggleVisualizationMode(); });

      noiseIndicator.addEventListener('click', calibrateNoise);
      noiseIndicator.addEventListener('touchend', (e) => { e.preventDefault(); calibrateNoise(); });

      document.addEventListener('keydown', (e) => {
        if (e.code === 'Space' && !e.repeat) { e.preventDefault(); handleRecordButtonClick(); }
        if (e.code === 'KeyR') clearSelection();
        if (e.code === 'KeyW') toggleVisualizationMode();
        if (e.code === 'KeyF') enterFullscreen();
        if (e.code === 'KeyN') calibrateNoise();
      });

      canvas.addEventListener('click', handleCanvasClick);
      canvas.addEventListener('touchstart', handleTouchStart);
      canvas.addEventListener('touchmove', handleTouchMove);
      canvas.addEventListener('touchend', handleTouchEnd);
      spectrogram.addEventListener('click', handleCanvasClick);
      spectrogram.addEventListener('touchstart', handleTouchStart);
      spectrogram.addEventListener('touchmove', handleTouchMove);
      spectrogram.addEventListener('touchend', handleTouchEnd);

      let lastTap = 0;
      document.addEventListener('touchend', (e) => {
        const currentTime = new Date().getTime();
        const tapGap = currentTime - lastTap;
        if (tapGap < 300 && tapGap > 0 && !document.fullscreenElement) {
          const y = e.changedTouches[0].clientY;
          if (y < window.innerHeight - 100) {
            enterFullscreen();
          }
        }
        lastTap = currentTime;
      });
    }

    function resizeCanvas() {
      canvas.width = window.innerWidth; canvas.height = window.innerHeight;
      spectrogram.width = window.innerWidth; spectrogram.height = window.innerHeight;
      drawWaveform(); if (audioBuffer && !isWaveformMode) drawSpectrogram();
    }

    function resumeAudioContext() { if (audioContext && audioContext.state === 'suspended') audioContext.resume(); }
    function updateStatus(message) { statusEl.textContent = message; console.log(message); }

    function enterFullscreen() {
      document.documentElement.requestFullscreen().catch(err => console.error('Fullscreen error:', err));
      document.body.classList.add('fullscreen');
    }

    function toggleVisualizationMode() {
      isWaveformMode = !isWaveformMode;
      if (isWaveformMode) {
        canvas.style.display = 'block'; spectrogram.style.display = 'none'; modeIndicator.textContent = 'W';
      } else {
        canvas.style.display = 'none'; spectrogram.style.display = 'block'; modeIndicator.textContent = 'S';
        if (audioBuffer) drawSpectrogram();
      }
    }

    async function handleRecordButtonClick() {
      if (audioContext.state === 'suspended') await audioContext.resume();
      if (!isRecording) startRecording(); else stopRecording();
    }

    async function calibrateNoise() {
      if (isRecording || isCalibrating) return;
      try {
        updateStatus('Calibrating noise (2s)...');
        isCalibrating = true;
        noiseIndicator.classList.add('calibrating');
        audioChunks = [];
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(mediaStream);
        mediaRecorder.ondataavailable = (event) => { if (event.data && event.data.size > 0) audioChunks.push(event.data); };
        mediaRecorder.start(1000);
        setTimeout(() => {
          mediaRecorder.stop();
          mediaStream.getTracks().forEach(track => track.stop());
          noiseIndicator.classList.remove('calibrating');
          processNoiseProfile();
        }, 2000);
      } catch (err) { updateStatus('Calibration failed: ' + err.message); console.error('Error:', err); isCalibrating = false; noiseIndicator.classList.remove('calibrating'); }
    }

    async function processNoiseProfile() {
      try {
        const blob = new Blob(audioChunks, { type: 'audio/wav' });
        const arrayBuffer = await blob.arrayBuffer();
        const noiseBuffer = await audioContext.decodeAudioData(arrayBuffer);
        noiseProfile = computeNoiseSpectrum(noiseBuffer);
        updateStatus('Noise calibrated - Ready');
        isCalibrating = false;
      } catch (err) { updateStatus('Noise calibration failed: ' + err.message); console.error('Error:', err); isCalibrating = false; }
    }

    function computeNoiseSpectrum(buffer) {
      const fftSize = 2048;
      const channelData = buffer.getChannelData(0);
      const spectrum = new Float32Array(fftSize / 2);
      let sum = new Float32Array(fftSize / 2);
      let count = 0;
      for (let offset = 0; offset + fftSize <= channelData.length; offset += fftSize / 2) {
        const real = new Float32Array(fftSize), imag = new Float32Array(fftSize);
        for (let i = 0; i < fftSize; i++) real[i] = channelData[offset + i] * (0.5 * (1 - Math.cos(2 * Math.PI * i / (fftSize - 1))));
        fft(real, imag);
        for (let i = 0; i < fftSize / 2; i++) {
          sum[i] += Math.sqrt(real[i] * real[i] + imag[i] * imag[i]);
        }
        count++;
      }
      for (let i = 0; i < fftSize / 2; i++) spectrum[i] = sum[i] / count;
      return spectrum;
    }

    async function startRecording() {
      try {
        audioChunks = [];
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) throw new Error('getUserMedia not supported');
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(mediaStream);
        mediaRecorder.ondataavailable = (event) => { if (event.data && event.data.size > 0) audioChunks.push(event.data); };
        mediaRecorder.start(1000);
        isRecording = true;
        recordButton.classList.add('recording');
        updateStatus('Recording');
      } catch (err) { updateStatus('Recording failed: ' + err.message); console.error('Error:', err); }
    }

    function stopRecording() {
      if (!isRecording || !mediaRecorder) return;
      updateStatus('Processing...');
      return new Promise((resolve, reject) => {
        mediaRecorder.onstop = async () => {
          try {
            if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
            if (audioChunks.length === 0) throw new Error('No audio data recorded');
            const blob = new Blob(audioChunks, { type: 'audio/wav' });
            if (blob.size < 44) throw new Error('Audio data too small to decode');
            console.log('Blob size:', blob.size);
            await processAudioBlob(blob);
            isRecording = false;
            recordButton.classList.remove('recording');
            updateStatus('Ready');
            resolve(blob);
          } catch (err) { updateStatus('Error: ' + err.message); console.error('Decoding error:', err); isRecording = false; recordButton.classList.remove('recording'); reject(err); }
        };
        setTimeout(() => {
          try { mediaRecorder.stop(); } catch (err) { if (mediaStream) mediaStream.getTracks().forEach(track => track.stop()); isRecording = false; recordButton.classList.remove('recording'); updateStatus('Error stopping: ' + err.message); reject(err); }
        }, 100);
      });
    }

    async function processAudioBlob(blob) {
      updateStatus('Processing audio...');
      try {
        const arrayBuffer = await blob.arrayBuffer();
        const rawBuffer = await audioContext.decodeAudioData(arrayBuffer);
        if (noiseProfile) {
          audioBuffer = await denoiseAudio(rawBuffer, noiseProfile);
        } else {
          audioBuffer = rawBuffer;
        }
        drawWaveform();
        selectedRegion = { start: 0, end: audioBuffer.duration };
        updateStatus('Ready - ' + Math.round(audioBuffer.duration * 1000) + 'ms');
        if (!isWaveformMode) drawSpectrogram();
      } catch (err) {
        console.error('DecodeAudioData failed:', err);
        throw new Error('Decoding failed');
      }
    }

    async function denoiseAudio(buffer, noiseSpectrum) {
      const fftSize = 2048;
      const channelData = buffer.getChannelData(0);
      const outputData = new Float32Array(channelData.length);

      for (let offset = 0; offset + fftSize <= channelData.length; offset += fftSize / 2) {
        const real = new Float32Array(fftSize), imag = new Float32Array(fftSize);
        for (let i = 0; i < fftSize; i++) real[i] = channelData[offset + i] * (0.5 * (1 - Math.cos(2 * Math.PI * i / (fftSize - 1))));
        fft(real, imag);

        const filteredReal = new Float32Array(fftSize), filteredImag = new Float32Array(fftSize);
        for (let i = 0; i < fftSize / 2; i++) {
          const magnitude = Math.sqrt(real[i] * real[i] + imag[i] * imag[i]);
          const phase = Math.atan2(imag[i], real[i]);
          let cleanMag = Math.max(0, magnitude - noiseSpectrum[i]);
          cleanMag = kalmanFactor * cleanMag + (1 - kalmanFactor) * magnitude;
          filteredReal[i] = cleanMag * Math.cos(phase);
          filteredImag[i] = cleanMag * Math.sin(phase);
          filteredReal[fftSize - 1 - i] = filteredReal[i];
          filteredImag[fftSize - 1 - i] = -filteredImag[i];
        }

        ifft(filteredReal, filteredImag);

        for (let i = 0; i < fftSize; i++) {
          if (offset + i < outputData.length) {
            outputData[offset + i] += filteredReal[i] * (0.5 * (1 - Math.cos(2 * Math.PI * i / (fftSize - 1))));
          }
        }
      }

      const cleanBuffer = audioContext.createBuffer(1, channelData.length, audioContext.sampleRate);
      cleanBuffer.getChannelData(0).set(outputData);
      return cleanBuffer;
    }

    function fft(real, imag) {
      const n = real.length;
      if (n === 1) return;
      const evenReal = new Float32Array(n / 2), evenImag = new Float32Array(n / 2);
      const oddReal = new Float32Array(n / 2), oddImag = new Float32Array(n / 2);
      for (let i = 0; i < n / 2; i++) {
        evenReal[i] = real[2 * i]; evenImag[i] = imag[2 * i];
        oddReal[i] = real[2 * i + 1]; oddImag[i] = imag[2 * i + 1];
      }
      fft(evenReal, evenImag); fft(oddReal, oddImag);
      for (let k = 0; k < n / 2; k++) {
        const theta = -2 * Math.PI * k / n, cosT = Math.cos(theta), sinT = Math.sin(theta);
        const tReal = cosT * oddReal[k] - sinT * oddImag[k], tImag = sinT * oddReal[k] + cosT * oddImag[k];
        real[k] = evenReal[k] + tReal; imag[k] = evenImag[k] + tImag;
        real[k + n / 2] = evenReal[k] - tReal; imag[k + n / 2] = evenImag[k] - tImag;
      }
    }

    function ifft(real, imag) {
      const n = real.length;
      for (let i = 0; i < n; i++) imag[i] = -imag[i];
      fft(real, imag);
      for (let i = 0; i < n; i++) {
        real[i] /= n;
        imag[i] = -imag[i] / n;
      }
    }

    function drawWaveform() {
      if (!audioBuffer || !ctx) return;
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      const channelData = audioBuffer.getChannelData(0), step = Math.ceil(channelData.length / canvas.width), amp = canvas.height / 2;
      ctx.beginPath(); ctx.moveTo(0, amp);
      for (let i = 0; i < canvas.width; i++) {
        let min = 1.0, max = -1.0;
        for (let j = 0; j < step; j++) {
          const datum = channelData[(i * step) + j]; if (datum < min) min = datum; if (datum > max) max = datum;
        }
        ctx.lineTo(i, (1 + min) * amp); ctx.lineTo(i, (1 + max) * amp);
      }
      ctx.strokeStyle = 'white'; ctx.lineWidth = 1; ctx.stroke();
      if (!isSilenced) drawRegion();
    }

    function drawSpectrogram() {
      if (!audioBuffer || !analyser) return;
      spectrogramCtx.clearRect(0, 0, spectrogram.width, spectrogram.height);
      const channelData = audioBuffer.getChannelData(0), duration = audioBuffer.duration;
      const samplesPerPixel = Math.floor(channelData.length / spectrogram.width), fftSize = analyser.fftSize;
      const dataArray = new Uint8Array(fftSize / 2), imageData = spectrogramCtx.createImageData(spectrogram.width, spectrogram.height);
      for (let x = 0; x < spectrogram.width; x++) {
        const startSample = x * samplesPerPixel, endSample = Math.min(startSample + fftSize, channelData.length);
        let fftInput = channelData.slice(startSample, endSample);
        if (fftInput.length < fftSize) {
          const padded = new Float32Array(fftSize); padded.set(fftInput); fftInput = padded;
        }
        const real = new Float32Array(fftSize), imag = new Float32Array(fftSize);
        for (let i = 0; i < fftSize; i++) real[i] = fftInput[i] * (0.5 * (1 - Math.cos(2 * Math.PI * i / (fftSize - 1))));
        fft(real, imag);
        for (let i = 0; i < fftSize / 2; i++) {
          const magnitude = Math.sqrt(real[i] * real[i] + imag[i] * imag[i]);
          const normalized = Math.min(255, Math.max(0, Math.round(magnitude * 255)));
          const y = spectrogram.height - 1 - Math.floor(i * spectrogram.height / (fftSize / 2));
          const idx = (y * spectrogram.width + x) * 4;
          imageData.data[idx] = imageData.data[idx + 1] = imageData.data[idx + 2] = normalized; imageData.data[idx + 3] = 255;
        }
      }
      spectrogramCtx.putImageData(imageData, 0, 0);
    }

    function drawRegion() {
      if (!audioBuffer) return;
      const existingRegion = document.querySelector('.region'); if (existingRegion) existingRegion.remove();
      const duration = audioBuffer.duration;
      const startPos = (selectedRegion.start / duration) * canvas.width;
      const endPos = (selectedRegion.end / duration) * canvas.width;
      if (startPos !== endPos) {
        const region = document.createElement('div');
        region.className = 'region';
        region.style.left = Math.min(startPos, endPos) + 'px';
        region.style.width = Math.abs(endPos - startPos) + 'px';
        const startHandle = document.createElement('div');
        startHandle.className = 'handle handle-start';
        startHandle.addEventListener('mousedown', (e) => handleResize(e, 'start'));
        startHandle.addEventListener('touchstart', (e) => handleResize(e, 'start'));
        const endHandle = document.createElement('div');
        endHandle.className = 'handle handle-end';
        endHandle.addEventListener('mousedown', (e) => handleResize(e, 'end'));
        endHandle.addEventListener('touchstart', (e) => handleResize(e, 'end'));
        region.appendChild(startHandle);
        region.appendChild(endHandle);
        waveformEl.appendChild(region);
        playSelection();
      }
    }

    function handleCanvasClick(e) {
      if (!audioBuffer) return;
      const rect = e.target.getBoundingClientRect(), x = e.clientX - rect.left;
      const clickTime = (x / rect.width) * audioBuffer.duration;
      if (isFirstClick) {
        firstClickTime = clickTime;
        selectedRegion.start = clickTime;
        selectedRegion.end = clickTime;
        isFirstClick = false;
        updateStatus('Start: ' + Math.round(clickTime * 1000) + 'ms');
      } else {
        selectedRegion.end = clickTime;
        if (selectedRegion.end < selectedRegion.start) {
          const temp = selectedRegion.start;
          selectedRegion.start = selectedRegion.end;
          selectedRegion.end = temp;
        }
        isFirstClick = true;
        isSilenced = false;
        drawRegion();
        updateStatus('Selection: ' + Math.round((selectedRegion.end - selectedRegion.start) * 1000) + 'ms');
      }
    }

    let touchStart = null;
    function handleTouchStart(e) {
      if (!audioBuffer) return;
      e.preventDefault();
      const rect = e.target.getBoundingClientRect();
      if (e.touches.length === 3) {
        const x1 = Math.round(e.touches[0].clientX - rect.left);
        const y1 = Math.round(e.touches[0].clientY - rect.top);
        const x2 = Math.round(e.touches[1].clientX - rect.left);
        const y2 = Math.round(e.touches[1].clientY - rect.top);
        const x3 = Math.round(e.touches[2].clientX - rect.left);
        const y3 = Math.round(e.touches[2].clientY - rect.top);
        updateStatus(`THREE FINGERS ${x1} ${y1} / ${x2} ${y2} / ${x3} ${y3}`);
      } else if (e.touches.length === 2) {
        const x1 = Math.round(e.touches[0].clientX - rect.left);
        const y1 = Math.round(e.touches[0].clientY - rect.top);
        const x2 = Math.round(e.touches[1].clientX - rect.left);
        const y2 = Math.round(e.touches[1].clientY - rect.top);
        twoFingerDistance = Math.sqrt(Math.pow(x2 - x1, 2) + Math.pow(y2 - y1, 2));
        updateStatus(`TWO FINGERS ${x1} ${y1} / ${x2} ${y2} - PINCH DISTANCE: ${Math.round(twoFingerDistance)}`);
      } else if (e.touches.length === 1) {
        const touch = e.touches[0], y = touch.clientY;
        if (y > window.innerHeight - 100 && Math.abs(touch.clientX - rect.left - canvas.width / 2) > 40) {
          fadeOutAudioAndHandlers();
        } else {
          touchStart = { x: touch.clientX - rect.left, time: (touch.clientX - rect.left) / rect.width * audioBuffer.duration };
          const force = touch.force !== undefined ? touch.force.toFixed(1) : 'N/A';
          updateStatus(`Start: ${Math.round((touch.clientX - rect.left) / rect.width * audioBuffer.duration)}ms - FORCE: ${force}`);
        }
      }
    }

    function handleTouchMove(e) {
      if (!audioBuffer) return;
      e.preventDefault();
      const rect = e.target.getBoundingClientRect();
      if (e.touches.length === 3) {
        const x1 = Math.round(e.touches[0].clientX - rect.left);
        const y1 = Math.round(e.touches[0].clientY - rect.top);
        const x2 = Math.round(e.touches[1].clientX - rect.left);
        const y2 = Math.round(e.touches[1].clientY - rect.top);
        const x3 = Math.round(e.touches[2].clientX - rect.left);
        const y3 = Math.round(e.touches[2].clientY - rect.top);
        updateStatus(`THREE FINGERS ${x1} ${y1} / ${x2} ${y2} / ${x3} ${y3}`);
      } else if (e.touches.length === 2) {
        const x1 = Math.round(e.touches[0].clientX - rect.left);
        const y1 = Math.round(e.touches[0].clientY - rect.top);
        const x2 = Math.round(e.touches[1].clientX - rect.left);
        const y2 = Math.round(e.touches[1].clientY - rect.top);
        const newDistance = Math.sqrt(Math.pow(x2 - x1, 2) + Math.pow(y2 - y1, 2));
        updateStatus(`TWO FINGERS ${x1} ${y1} / ${x2} ${y2} - PINCH DISTANCE: ${Math.round(newDistance)}`);
        twoFingerDistance = newDistance;
      } else if (touchStart) {
        const touch = e.touches[0], x = touch.clientX - rect.left;
        const touchTime = (x / rect.width) * audioBuffer.duration;
        selectedRegion.start = Math.min(touchStart.time, touchTime);
        selectedRegion.end = Math.max(touchStart.time, touchTime);
        drawRegion();
      }
    }

    function handleTouchEnd(e) {
      if (e.touches.length < 2) {
        twoFingerDistance = null;
        if (touchStart) {
          updateStatus('Selection: ' + Math.round((selectedRegion.end - selectedRegion.start) * 1000) + 'ms');
          touchStart = null;
        } else {
          updateStatus('Ready');
        }
      }
    }

    let resizeType = null, startX = 0;
    function handleResize(e, type) {
      e.preventDefault(); e.stopPropagation();
      resizeType = type;
      if (e.type === 'mousedown') startX = e.clientX; else startX = e.touches[0].clientX;
      document.addEventListener('mousemove', handleResizeMove);
      document.addEventListener('touchmove', handleResizeMove);
      document.addEventListener('mouseup', stopResize);
      document.addEventListener('touchend', stopResize);
    }

    function handleResizeMove(e) {
      if (!resizeType || !audioBuffer) return;
      e.preventDefault();
      let currentX = e.type === 'mousemove' ? e.clientX : e.touches[0].clientX;
      const rect = canvas.getBoundingClientRect();
      const delta = (currentX - startX) / rect.width * audioBuffer.duration;
      if (resizeType === 'start') {
        selectedRegion.start = Math.max(0, Math.min(selectedRegion.start + delta, selectedRegion.end - 0.01));
      } else {
        selectedRegion.end = Math.min(audioBuffer.duration, Math.max(selectedRegion.start + 0.01, selectedRegion.end + delta));
      }
      drawRegion();
      startX = currentX;
    }

    function stopResize() {
      resizeType = null;
      document.removeEventListener('mousemove', handleResizeMove);
      document.removeEventListener('touchmove', handleResizeMove);
      document.removeEventListener('mouseup', stopResize);
      document.removeEventListener('touchend', stopResize);
      updateStatus('Selection: ' + Math.round((selectedRegion.end - selectedRegion.start) * 1000) + 'ms');
      playSelection();
    }

    function clearSelection() {
      if (!audioBuffer) return;
      selectedRegion = { start: 0, end: audioBuffer.duration };
      isFirstClick = true;
      firstClickTime = null;
      const existingRegion = document.querySelector('.region');
      if (existingRegion) existingRegion.remove();
      stopPlayback();
      updateStatus('Selection cleared');
    }

    function playSelection(silentMode = false) {
      if (!audioBuffer) return;
      stopPlayback();
      try {
        sourceNode = audioContext.createBufferSource();
        sourceNode.buffer = audioBuffer;
        sourceNode.connect(analyser);
        sourceNode.connect(gainNode);
        if (!silentMode) gainNode.connect(audioContext.destination);
        sourceNode.loop = true;
        sourceNode.loopStart = Math.max(0, selectedRegion.start);
        sourceNode.loopEnd = Math.min(selectedRegion.end, audioBuffer.duration);

        const loopDuration = sourceNode.loopEnd - sourceNode.loopStart;
        const fadeTime = 0.004;
        const currentTime = audioContext.currentTime;

        gainNode.gain.setValueAtTime(0, currentTime);
        gainNode.gain.linearRampToValueAtTime(1, currentTime + fadeTime);

        function scheduleFade() {
          if (!isPlaying) return;
          const nextLoopStart = currentTime + loopDuration;
          gainNode.gain.setValueAtTime(1, nextLoopStart - fadeTime);
          gainNode.gain.linearRampToValueAtTime(0, nextLoopStart);
          gainNode.gain.setValueAtTime(0, nextLoopStart);
          gainNode.gain.linearRampToValueAtTime(1, nextLoopStart + fadeTime);
          setTimeout(scheduleFade, (loopDuration - fadeTime) * 1000);
        }
        scheduleFade();

        gainNode.gain.value = 1;
        sourceNode.start(0, sourceNode.loopStart);
        isPlaying = true;
        if (!silentMode) updateStatus('Playing: ' + Math.round((selectedRegion.end - selectedRegion.start) * 1000) + 'ms');
        else updateStatus('Analyzing: ' + Math.round((selectedRegion.end - selectedRegion.start) * 1000) + 'ms');
      } catch (err) { updateStatus('Playback error: ' + err.message); }
    }

    function stopPlayback() {
      if (!isPlaying || !sourceNode) return;
      try { sourceNode.stop(); sourceNode.disconnect(); gainNode.disconnect(); sourceNode = null; isPlaying = false; updateStatus('Stopped'); } catch (err) { console.error('Error stopping playback:', err); }
    }

    function fadeOutAudioAndHandlers() {
      if (!isPlaying || !gainNode) return;
      const region = document.querySelector('.region'), handles = document.querySelectorAll('.handle');
      gainNode.gain.linearRampToValueAtTime(0, audioContext.currentTime + toFadeOut / 1000);
      if (region) region.style.opacity = '0'; handles.forEach(h => h.style.opacity = '0');
      setTimeout(() => { stopPlayback(); if (region) region.remove(); isSilenced = true; }, toFadeOut);
    }
  </script>
</body>
</html>